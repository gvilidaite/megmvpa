{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MVPA MEG Tutorial (Python): Distance measures and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Tutorial starting page](https://github.com/m-guggenmos/megmvpa/).\n",
    "\n",
    "This tutorial accompanies the preprint titled \"Multivariate pattern analysis for MEG: a comprehensive comparison of dissimilarity measures\", which is available at [doi.org/10.1101/172619](https://doi.org/10.1101/172619).\n",
    "\n",
    "**Goal of this tutorial:** comparison of Euclidean and Pearson distance measures + cross-validation.\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "We start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.discriminant_analysis import _cov\n",
    "\n",
    "from cv import ShuffleBinLeaveOneOut, ShuffleBin\n",
    "from dissimilarity import Euclidean2, CvEuclidean2, Pearson, CvPearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a seed, in order to make analyses reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = '' # This is the /path/to the directory containing the data.\n",
    "          # We leave it empty here, because the data is located in the current directory.\n",
    "\n",
    "# Load data and trial labels for the two sessions of participant 01\n",
    "sessions = [\n",
    "    # Session 1\n",
    "    dict(\n",
    "        data=np.load(os.path.join(root, 'data01_sess1.npy')),\n",
    "        # data has shape n_trials x n_sensors x n_timepoints\n",
    "        labels=np.load(os.path.join(root, 'labels01_sess1.npy'))\n",
    "        # labels has shape 1 x n_trials (i.e., one condition label [object category] per trial)\n",
    "    ),\n",
    "    # Session 2\n",
    "    dict(\n",
    "        data=np.load(os.path.join(root, 'data01_sess2.npy')),\n",
    "        labels=np.load(os.path.join(root, 'labels01_sess2.npy'))\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set some parameters. Only the number of permutations and the number of pseudo-trials are free parameters. The number of conditions, sensors, time points and sessions are derived from the data (i.e., from the `sessions` variable above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_perm = 20  # number of permutations\n",
    "n_pseudo = 5  # number of pseudo-trials\n",
    "n_conditions = len(np.unique(sessions[0]['labels']))\n",
    "n_sensors = sessions[0]['data'].shape[1]\n",
    "n_time = sessions[0]['data'].shape[2]\n",
    "n_sessions = len(sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our distance measures: non-cross and cross-validated variants of the (squared) Euclidean and the Pearson distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ec = Euclidean2()\n",
    "ec_cv = CvEuclidean2()\n",
    "ps = Pearson()\n",
    "ps_cv = CvPearson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-cross-validated distances\n",
    "\n",
    "We start with computing non-cross-validated distances (`ec` and `ps`). For this, the analytic logic is contained in a nested for loop, with loops for the number of sessions, number of permutations, number of timepoints, number of conditions, and number of conditions again. Overall, the logic contains 3 crucial steps:\n",
    "1. Compute pseudo-trials for the training and test data\n",
    "2. Whiten the training data (here using the Epoch method, which is recommended in our manuscript)\n",
    "3. Apply distance measure to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result_distance.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-688342cd5076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreload_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# for recomputing the decoding analyses, set to False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreload_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result_distance.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     result = np.full((n_sessions, n_perm, n_conditions, n_conditions, n_time), np.nan,\n",
      "\u001b[0;32m/home/matteo/python/anaconda3.6/envs/neuro/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result_distance.npy'"
     ]
    }
   ],
   "source": [
    "# pre-load mechanism, for convenience\n",
    "preload_result = True # for recomputing the decoding analyses, set to False\n",
    "if preload_result:\n",
    "    result = np.load(os.path.join(root, 'result_distance.npy'))\n",
    "else:\n",
    "    result = np.full((n_sessions, n_perm, n_conditions, n_conditions, n_time), np.nan,\n",
    "                     dtype={'names': ['ec', 'ps'], 'formats': 2*['f8']})\n",
    "    for s, session in enumerate(sessions):\n",
    "\n",
    "        print('Session %g / %g' % (s + 1, n_sessions))\n",
    "\n",
    "        X = session['data']\n",
    "        y = session['labels']\n",
    "\n",
    "        cv = ShuffleBin(y, n_iter=n_perm, n_pseudo=n_pseudo)\n",
    "\n",
    "        for f, (train_indices, test_indices) in enumerate(cv.split(X)):\n",
    "            print('\\tPermutation %g / %g' % (f + 1, n_perm))\n",
    "\n",
    "            # 1. Compute pseudo-trials\n",
    "            Xpseudo = np.full((len(test_indices), n_sensors, n_time), np.nan)\n",
    "            for i, ind in enumerate(test_indices):\n",
    "                Xpseudo[i, :, :] = np.mean(X[ind, :, :], axis=0)\n",
    "\n",
    "            # 2. Whitening using the Epoch method\n",
    "            sigma_conditions = cv.labels_pseudo_test[0, :, n_pseudo:].flatten()\n",
    "            sigma_ = np.empty((n_conditions, n_sensors, n_sensors))\n",
    "            for c in range(n_conditions):\n",
    "                # compute sigma for each time point, then average across time\n",
    "                sigma_[c] = np.mean([_cov(Xpseudo[sigma_conditions==c, :, t], shrinkage='auto')\n",
    "                                     for t in range(n_time)], axis=0)\n",
    "            sigma = sigma_.mean(axis=0)  # average across conditions\n",
    "            sigma_inv = scipy.linalg.fractional_matrix_power(sigma, -0.5)\n",
    "            Xpseudo = (Xpseudo.swapaxes(1, 2) @ sigma_inv).swapaxes(1, 2)\n",
    "\n",
    "            for t in range(n_time):\n",
    "                for c1 in range(n_conditions-1):\n",
    "                    for c2 in range(min(c1 + 1, n_conditions-1), n_conditions):\n",
    "                            # 3. Apply distance measure to the data\n",
    "                            data = Xpseudo[cv.ind_pseudo_test[c1, c2], :, t]\n",
    "                            result['ec'][s, f, c1, c2, t] = ec.predict(data, y=cv.labels_pseudo_test[c1, c2])\n",
    "                            result['ps'][s, f, c1, c2, t] = ps.predict(data, y=cv.labels_pseudo_test[c1, c2])\n",
    "    # average across permutations\n",
    "    result_ = np.full((n_sessions, n_conditions, n_conditions, n_time), np.nan,\n",
    "                      dtype={'names': ['ec', 'ps'], 'formats': 2*['f8']})\n",
    "    result_['ec'] = np.nanmean(result['ec'], axis=1)\n",
    "    result_['ps'] = np.nanmean(result['ps'], axis=1)\n",
    "    result = result_\n",
    "    np.save(os.path.join(root, 'result_distance.npy'), result_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a look at the results, we use matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(np.arange(-100, 1001, 10), np.nanmean(result['ec'], axis=(0, 1, 2)), label='Euclidean')\n",
    "plt.xlim((-100, 1000))\n",
    "plt.ylabel('Euclidean distance', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(-100, 1001, 10), np.nanmean(result['ps'], axis=(0, 1, 2)), label='Pearson')\n",
    "plt.xlim((-100, 1000))\n",
    "plt.xlabel('Time [ms]', fontsize=12)\n",
    "plt.ylabel('Pearson distance', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euclidean distance:** the Euclidean distance rises shortly after stimulus onset (at 0ms) and until it reaches a maximum. Interestingly, the distance remains at this maximum after stimulus offset (at 500ms) until the end of the epoch, although the condition-specific information content [drops to zero](https://github.com/m-guggenmos/megmvpa/blob/master/tutorial_python/python_decoding.ipynb) towards the end of the trial. This indicates that the non-cross-validated Euclidean distance is not only influenced by condition-specific differences in activation patterns, but also by noise.\n",
    "\n",
    "**Pearson distance:** the Pearson distance _decreases_ after stimulus onset, corresponding to an increase of the correlation between condition-specific activation patterns (note that the Pearson distance is defined as 1−r). Thus, activation patterns in fact get more similar after stimulus onset, which intuitively seems surprising, since the [decoding analysis](https://github.com/m-guggenmos/megmvpa/blob/master/tutorial_python/python_decoding.ipynb) shows that they also get more distinct. In contrast to the Euclidean distance, the Pearson approaches its baseline value towards the end of the trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross-validated distances\n",
    "\n",
    "We now turn to cross-validated measures. Here too, the analytic logic is contained in a nested for loop, with loops for the number of sessions, number of permutations, number of timepoints, number of conditions, and number of conditions again. Overall, the logic contains 4 crucial steps:\n",
    "1. Compute pseudo-trials for the training and test data\n",
    "2. Whiten the training data (here using the Epoch method, which is recommended in our manuscript)\n",
    "3. Apply distance measure to training data\n",
    "4. Validate distance measure on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-load mechanism, for convenience\n",
    "preload_result = True # for recomputing the decoding analyses, set to False\n",
    "if preload_result:\n",
    "    result_cv = np.load(os.path.join(root, 'result_distance_cv.npy'))\n",
    "else:\n",
    "    result_cv = np.full((n_sessions, n_perm, n_conditions, n_conditions, n_time), np.nan,\n",
    "                        dtype={'names': ['ec_cv', 'ps_cv'], 'formats': 2*['f8']})\n",
    "    for s, session in enumerate(sessions):\n",
    "\n",
    "        print('Session %g / %g' % (s + 1, n_sessions))\n",
    "\n",
    "        X = session['data']\n",
    "        y = session['labels']\n",
    "\n",
    "        cv = ShuffleBinLeaveOneOut(y, n_iter=n_perm, n_pseudo=n_pseudo)\n",
    "\n",
    "        for f, (train_indices, test_indices) in enumerate(cv.split(X)):\n",
    "            print('\\tPermutation %g / %g' % (f + 1, n_perm))\n",
    "\n",
    "            # 1. Compute pseudo-trials for training and test\n",
    "            Xpseudo_train = np.full((len(train_indices), n_sensors, n_time), np.nan)\n",
    "            Xpseudo_test = np.full((len(test_indices), n_sensors, n_time), np.nan)\n",
    "            for i, ind in enumerate(train_indices):\n",
    "                Xpseudo_train[i, :, :] = np.mean(X[ind, :, :], axis=0)\n",
    "            for i, ind in enumerate(test_indices):\n",
    "                Xpseudo_test[i, :, :] = np.mean(X[ind, :, :], axis=0)\n",
    "\n",
    "\n",
    "            # 2. Whitening using the Epoch method\n",
    "            sigma_conditions = cv.labels_pseudo_train[0, :, n_pseudo-1:].flatten()\n",
    "            sigma_ = np.empty((n_conditions, n_sensors, n_sensors))\n",
    "            for c in range(n_conditions):\n",
    "                # compute sigma for each time point, then average across time\n",
    "                sigma_[c] = np.mean([_cov(Xpseudo_train[sigma_conditions==c, :, t], shrinkage='auto')\n",
    "                                     for t in range(n_time)], axis=0)\n",
    "            sigma = sigma_.mean(axis=0)  # average across conditions\n",
    "            sigma_inv = scipy.linalg.fractional_matrix_power(sigma, -0.5)\n",
    "            Xpseudo_train = (Xpseudo_train.swapaxes(1, 2) @ sigma_inv).swapaxes(1, 2)\n",
    "            Xpseudo_test = (Xpseudo_test.swapaxes(1, 2) @ sigma_inv).swapaxes(1, 2)\n",
    "\n",
    "            for t in range(n_time):\n",
    "                for c1 in range(n_conditions-1):\n",
    "                    for c2 in range(min(c1 + 1, n_conditions-1), n_conditions):\n",
    "                            # 3. Apply distance measure to training data\n",
    "                            data_train = Xpseudo_train[cv.ind_pseudo_train[c1, c2], :, t]\n",
    "                            ec_cv.fit(data_train, cv.labels_pseudo_train[c1, c2])                            \n",
    "                            ps_cv.fit(data_train, cv.labels_pseudo_train[c1, c2])\n",
    "\n",
    "                            # 4. Validate distance measure on testing data\n",
    "                            data_test = Xpseudo_test[cv.ind_pseudo_test[c1, c2], :, t]\n",
    "                            result_cv['ec_cv'][s, f, c1, c2, t] = ec_cv.predict(data_test, y=cv.labels_pseudo_test[c1, c2])\n",
    "                            result_cv['ps_cv'][s, f, c1, c2, t] = ps_cv.predict(data_test, y=cv.labels_pseudo_test[c1, c2])\n",
    "    # average across permutations\n",
    "    result_cv_ = np.full((n_sessions, n_conditions, n_conditions, n_time), np.nan,\n",
    "                      dtype={'names': ['ec_cv', 'ps_cv'], 'formats': 2*['f8']})\n",
    "    result_cv_['ec_cv'] = np.nanmean(result_cv['ec_cv'], axis=1)\n",
    "    result_cv_['ps_cv'] = np.nanmean(result_cv['ps_cv'], axis=1)\n",
    "    result_cv = result_cv_\n",
    "    np.save(os.path.join(root, 'result_distance_cv.npy'), result_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the cross-validated distances on top of the non-cross-validated distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(np.arange(-100, 1001, 10), np.nanmean(result['ec'], axis=(0, 1, 2)), label='Euclidean')\n",
    "plt.plot(np.arange(-100, 1001, 10), np.nanmean(result_cv['ec_cv'], axis=(0, 1, 2)), label='Euclidean (c.v.)')\n",
    "plt.xlim((-100, 1000))\n",
    "plt.ylabel('Euclidean distance', fontsize=12)\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(-100, 1001, 10), np.nanmean(result['ps'], axis=(0, 1, 2)), label='Pearson', clip_on=False)\n",
    "plt.plot(np.arange(-100, 1001, 10), np.nanmean(result_cv['ps_cv'], axis=(0, 1, 2)), label='Pearson (c.v.)', clip_on=False)\n",
    "plt.xlim((-100, 1000))\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('Time [ms]', fontsize=12)\n",
    "plt.ylabel('Pearson distance', fontsize=12)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euclidean distance:** with cross-validation, the Euclidean differences now approaches zero towards the end of an epoch. Thus, cross-validation removes the noise bias, which previously led to high Euclidean distances at the end of a trial despite the absence of true activation pattern differences (as indicated by decoding analyses).\n",
    "\n",
    "**Pearson distance:** with cross-validation, correlation coefficients increase and thus the Pearson distance (1−r) shows a stronger negative deflection from 1. This result is expected, as 1) cross-validation corrects for effects of noise and 2) noise generally decreases correlation coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
